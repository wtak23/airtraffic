{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Aim-of-notebook\" data-toc-modified-id=\"Aim-of-notebook-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Aim of notebook</a></div><div class=\"lev1 toc-item\"><a href=\"#Load-the-main-airport-traffic-data\" data-toc-modified-id=\"Load-the-main-airport-traffic-data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load the main airport traffic data</a></div><div class=\"lev1 toc-item\"><a href=\"#Load-lookup-table-provided-by-BTS\" data-toc-modified-id=\"Load-lookup-table-provided-by-BTS-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Load lookup table provided by BTS</a></div><div class=\"lev1 toc-item\"><a href=\"#Create-enhanced-lookuptable\" data-toc-modified-id=\"Create-enhanced-lookuptable-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Create <em>enhanced</em> lookuptable</a></div><div class=\"lev2 toc-item\"><a href=\"#Remove-Code-that-is-not-present-our-dataset\" data-toc-modified-id=\"Remove-Code-that-is-not-present-our-dataset-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Remove Code that is not present our dataset</a></div><div class=\"lev2 toc-item\"><a href=\"#Parse-state,city,-and-airport-name-from-'Description'-field\" data-toc-modified-id=\"Parse-state,city,-and-airport-name-from-'Description'-field-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Parse state,city, and airport-name from 'Description' field</a></div><div class=\"lev2 toc-item\"><a href=\"#Add-state-'region'-information\" data-toc-modified-id=\"Add-state-'region'-information-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Add state 'region' information</a></div><div class=\"lev2 toc-item\"><a href=\"#Add-airport-latitude-and-longitude-information-using-geocoder\" data-toc-modified-id=\"Add-airport-latitude-and-longitude-information-using-geocoder-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Add airport latitude and longitude information using geocoder</a></div><div class=\"lev2 toc-item\"><a href=\"#Add-column-with-both-city-and-state\" data-toc-modified-id=\"Add-column-with-both-city-and-state-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Add column with both city and state</a></div><div class=\"lev1 toc-item\"><a href=\"#All-done.-Save-dataframe-on-disk\" data-toc-modified-id=\"All-done.-Save-dataframe-on-disk-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>All done. Save dataframe on disk</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from pprint import pprint\n",
    "from util import print_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim of notebook\n",
    "\n",
    "- The [airport traffic dataset](http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time) encodes the airport with unique ID numbers.\n",
    "\n",
    "- In this notebook, we'll create an *enhanced lookup-table* by taking the lookup table provided by the BTS ([download link](http://www.transtats.bts.gov/Download_Lookup.asp?Lookup=L_AIRPORT_ID)) and adding additional relevant information regarding the airport (such as latitude/longitutde info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the main airport traffic data\n",
    "\n",
    "- Load 3 years worth of air-traffic data provided by BTS ([link](http://www.transtats.bts.gov/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time))\n",
    "- (from November 2013 to October 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ... load dataframe from 2013-11.zip \n",
      " ... load dataframe from 2013-12.zip \n",
      " ... load dataframe from 2014-01.zip \n",
      " ... load dataframe from 2014-02.zip \n",
      " ... load dataframe from 2014-03.zip \n",
      " ... load dataframe from 2014-04.zip \n",
      " ... load dataframe from 2014-05.zip \n",
      " ... load dataframe from 2014-06.zip \n",
      " ... load dataframe from 2014-07.zip \n",
      " ... load dataframe from 2014-08.zip \n",
      " ... load dataframe from 2014-09.zip \n",
      " ... load dataframe from 2014-10.zip \n",
      " ... load dataframe from 2014-11.zip \n",
      " ... load dataframe from 2014-12.zip \n",
      " ... load dataframe from 2015-01.zip \n",
      " ... load dataframe from 2015-02.zip \n",
      " ... load dataframe from 2015-03.zip \n",
      " ... load dataframe from 2015-04.zip \n",
      " ... load dataframe from 2015-05.zip \n",
      " ... load dataframe from 2015-06.zip \n",
      " ... load dataframe from 2015-07.zip \n",
      " ... load dataframe from 2015-08.zip \n",
      " ... load dataframe from 2015-09.zip \n",
      " ... load dataframe from 2015-10.zip \n",
      " ... load dataframe from 2015-11.zip \n",
      " ... load dataframe from 2015-12.zip \n",
      " ... load dataframe from 2016-01.zip \n",
      " ... load dataframe from 2016-02.zip \n",
      " ... load dataframe from 2016-03.zip \n",
      " ... load dataframe from 2016-04.zip \n",
      " ... load dataframe from 2016-05.zip \n",
      " ... load dataframe from 2016-06.zip \n",
      " ... load dataframe from 2016-07.zip \n",
      " ... load dataframe from 2016-08.zip \n",
      " ... load dataframe from 2016-09.zip \n",
      " ... load dataframe from 2016-10.zip \n"
     ]
    }
   ],
   "source": [
    "from util import load_airport_data_3years\n",
    "df_data = load_airport_data_3years()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17364696, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YEAR</th>\n",
       "      <th>QUARTER</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY_OF_MONTH</th>\n",
       "      <th>DAY_OF_WEEK</th>\n",
       "      <th>ORIGIN_AIRPORT_ID</th>\n",
       "      <th>DEST_AIRPORT_ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>12478</td>\n",
       "      <td>10693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12478</td>\n",
       "      <td>10693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>12478</td>\n",
       "      <td>10693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>12478</td>\n",
       "      <td>10693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>12478</td>\n",
       "      <td>10693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   YEAR  QUARTER  MONTH  DAY_OF_MONTH  DAY_OF_WEEK  ORIGIN_AIRPORT_ID  \\\n",
       "0  2013        4     11             3            7              12478   \n",
       "1  2013        4     11             4            1              12478   \n",
       "2  2013        4     11             5            2              12478   \n",
       "3  2013        4     11             6            3              12478   \n",
       "4  2013        4     11             7            4              12478   \n",
       "\n",
       "   DEST_AIRPORT_ID  \n",
       "0            10693  \n",
       "1            10693  \n",
       "2            10693  \n",
       "3            10693  \n",
       "4            10693  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print df_data.shape\n",
    "df_data.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load lookup table provided by BTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6409, 2)\n"
     ]
    }
   ],
   "source": [
    "df_lookup = pd.read_csv('../data/L_AIRPORT_ID.csv')\n",
    "print df_lookup.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create *enhanced* lookuptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10001</td>\n",
       "      <td>Afognak Lake, AK: Afognak Lake Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10003</td>\n",
       "      <td>Granite Mountain, AK: Bear Creek Mining Strip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10004</td>\n",
       "      <td>Lik, AK: Lik Mining Camp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10005</td>\n",
       "      <td>Little Squaw, AK: Little Squaw Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10006</td>\n",
       "      <td>Kizhuyak, AK: Kizhuyak Bay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10007</td>\n",
       "      <td>Klawock, AK: Klawock Seaplane Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10008</td>\n",
       "      <td>Elizabeth Island, AK: Elizabeth Island Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10009</td>\n",
       "      <td>Homer, AK: Augustin Island</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10010</td>\n",
       "      <td>Hudson, NY: Columbia County</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10011</td>\n",
       "      <td>Peach Springs, AZ: Grand Canyon West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Code                                     Description\n",
       "0  10001          Afognak Lake, AK: Afognak Lake Airport\n",
       "1  10003   Granite Mountain, AK: Bear Creek Mining Strip\n",
       "2  10004                        Lik, AK: Lik Mining Camp\n",
       "3  10005          Little Squaw, AK: Little Squaw Airport\n",
       "4  10006                      Kizhuyak, AK: Kizhuyak Bay\n",
       "5  10007              Klawock, AK: Klawock Seaplane Base\n",
       "6  10008  Elizabeth Island, AK: Elizabeth Island Airport\n",
       "7  10009                      Homer, AK: Augustin Island\n",
       "8  10010                     Hudson, NY: Columbia County\n",
       "9  10011            Peach Springs, AZ: Grand Canyon West"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lookup.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Code that is not present our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6409 Airport-Codes in the lookup table\n",
      "There are 334 unique airport-codes in our dataset\n"
     ]
    }
   ],
   "source": [
    "# unique ID's in the dataset\n",
    "uniq_orig = df_data['ORIGIN_AIRPORT_ID'].unique().tolist() \n",
    "uniq_dest = df_data['DEST_AIRPORT_ID'].unique().tolist()\n",
    "\n",
    "# apply ``set`` function to get unique items in concatenated list\n",
    "uniq_id = list(set(uniq_orig + uniq_dest))\n",
    "\n",
    "print \"There are {} Airport-Codes in the lookup table\".format(df_lookup.shape[0])\n",
    "print \"There are {} unique airport-codes in our dataset\".format(uniq_id.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter/drop the rows/records that we do not need in our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(334, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10135</td>\n",
       "      <td>Allentown/Bethlehem/Easton, PA: Lehigh Valley ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10136</td>\n",
       "      <td>Abilene, TX: Abilene Regional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10140</td>\n",
       "      <td>Albuquerque, NM: Albuquerque International Sun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10141</td>\n",
       "      <td>Aberdeen, SD: Aberdeen Regional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10146</td>\n",
       "      <td>Albany, GA: Southwest Georgia Regional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10154</td>\n",
       "      <td>Nantucket, MA: Nantucket Memorial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10155</td>\n",
       "      <td>Waco, TX: Waco Regional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10157</td>\n",
       "      <td>Arcata/Eureka, CA: Arcata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10158</td>\n",
       "      <td>Atlantic City, NJ: Atlantic City International</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10165</td>\n",
       "      <td>Adak Island, AK: Adak</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Code                                        Description\n",
       "0  10135  Allentown/Bethlehem/Easton, PA: Lehigh Valley ...\n",
       "1  10136                      Abilene, TX: Abilene Regional\n",
       "2  10140  Albuquerque, NM: Albuquerque International Sun...\n",
       "3  10141                    Aberdeen, SD: Aberdeen Regional\n",
       "4  10146             Albany, GA: Southwest Georgia Regional\n",
       "5  10154                  Nantucket, MA: Nantucket Memorial\n",
       "6  10155                            Waco, TX: Waco Regional\n",
       "7  10157                          Arcata/Eureka, CA: Arcata\n",
       "8  10158     Atlantic City, NJ: Atlantic City International\n",
       "9  10165                              Adak Island, AK: Adak"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only keep the items in the main dataframe\n",
    "_mask = df_lookup['Code'].isin( uniq_id )\n",
    "df_lookup = df_lookup[ _mask ].reset_index(drop=True)\n",
    "\n",
    "print df_lookup.shape\n",
    "df_lookup.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse state,city, and airport-name from 'Description' field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above we realize that the ``Description`` field contains information regarding the *city*, *state*, and *name* of the airport.\n",
    "\n",
    "- Let's create individual field for each information.\n",
    "\n",
    "- Fortunately, the ``Description`` column uses a comma (``,``) and colon (``:``) to delimit the City, State, Airport-name information, so splitting these are is straightforward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Airport': 'Lehigh Valley International',\n",
      "  'City': 'Allentown/Bethlehem/Easton',\n",
      "  'State': 'PA'},\n",
      " {'Airport': 'Abilene Regional', 'City': 'Abilene', 'State': 'TX'},\n",
      " {'Airport': 'Albuquerque International Sunport',\n",
      "  'City': 'Albuquerque',\n",
      "  'State': 'NM'},\n",
      " {'Airport': 'Aberdeen Regional', 'City': 'Aberdeen', 'State': 'SD'},\n",
      " {'Airport': 'Southwest Georgia Regional', 'City': 'Albany', 'State': 'GA'}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airport</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lehigh Valley International</td>\n",
       "      <td>Allentown/Bethlehem/Easton</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abilene Regional</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Albuquerque International Sunport</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aberdeen Regional</td>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>SD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Southwest Georgia Regional</td>\n",
       "      <td>Albany</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Airport                        City State\n",
       "0        Lehigh Valley International  Allentown/Bethlehem/Easton    PA\n",
       "1                   Abilene Regional                     Abilene    TX\n",
       "2  Albuquerque International Sunport                 Albuquerque    NM\n",
       "3                  Aberdeen Regional                    Aberdeen    SD\n",
       "4         Southwest Georgia Regional                      Albany    GA"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply string \"split\" method to break information up\n",
    "df_parse = map(lambda splits: {'City':splits[0],'State':splits[2],'Airport':splits[4]},\n",
    "               df_lookup['Description'].str.split(r'(,\\s|:\\s)') )\n",
    "\n",
    "pprint(df_parse[:5])\n",
    "\n",
    "# convert dict to dataframe\n",
    "df_parse = pd.DataFrame(df_parse)\n",
    "df_parse.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(334, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Description</th>\n",
       "      <th>Airport</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10135</td>\n",
       "      <td>Allentown/Bethlehem/Easton, PA: Lehigh Valley ...</td>\n",
       "      <td>Lehigh Valley International</td>\n",
       "      <td>Allentown/Bethlehem/Easton</td>\n",
       "      <td>PA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10136</td>\n",
       "      <td>Abilene, TX: Abilene Regional</td>\n",
       "      <td>Abilene Regional</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10140</td>\n",
       "      <td>Albuquerque, NM: Albuquerque International Sun...</td>\n",
       "      <td>Albuquerque International Sunport</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>NM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10141</td>\n",
       "      <td>Aberdeen, SD: Aberdeen Regional</td>\n",
       "      <td>Aberdeen Regional</td>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>SD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10146</td>\n",
       "      <td>Albany, GA: Southwest Georgia Regional</td>\n",
       "      <td>Southwest Georgia Regional</td>\n",
       "      <td>Albany</td>\n",
       "      <td>GA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Code                                        Description  \\\n",
       "0  10135  Allentown/Bethlehem/Easton, PA: Lehigh Valley ...   \n",
       "1  10136                      Abilene, TX: Abilene Regional   \n",
       "2  10140  Albuquerque, NM: Albuquerque International Sun...   \n",
       "3  10141                    Aberdeen, SD: Aberdeen Regional   \n",
       "4  10146             Albany, GA: Southwest Georgia Regional   \n",
       "\n",
       "                             Airport                        City State  \n",
       "0        Lehigh Valley International  Allentown/Bethlehem/Easton    PA  \n",
       "1                   Abilene Regional                     Abilene    TX  \n",
       "2  Albuquerque International Sunport                 Albuquerque    NM  \n",
       "3                  Aberdeen Regional                    Aberdeen    SD  \n",
       "4         Southwest Georgia Regional                      Albany    GA  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we can readily add these information to our lookup table\n",
    "df_lookup = df_lookup.join(df_parse)\n",
    "\n",
    "print df_lookup.shape\n",
    "df_lookup.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add state 'region' information\n",
    "\n",
    "I also would like to study patterns among the four-regions in the United States: \n",
    "\n",
    "(1) Northeast\n",
    "(2) South\n",
    "(3) West\n",
    "(4) Midwest\n",
    "\n",
    "I saved a json lookup file for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"Northeast\" : [\"Connecticut\",\"Maine\", \"Massachusetts\", \"New Hampshire\", \"Rhode Island\", \"Vermont\",\"New Jersey\", \"New York\", \"Pennsylvania\"],\n",
      "\"Midwest\"   : [\"Illinois\", \"Indiana\", \"Michigan\", \"Ohio\", \"Wisconsin\", \"Iowa\", \"Kansas\", \"Minnesota\", \"Missouri\", \"Nebraska\", \"North Dakota\", \"South Dakota\"],\n",
      "\"South\"     : [ \"Delaware\", \"Florida\", \"Georgia\", \"Maryland\", \"North Carolina\", \"South Carolina\", \"Virginia\", \"District of Columbia\", \"West Virginia\",             \"Alabama\", \"Kentucky\", \"Mississippi\", \"Tennessee\",\"Arkansas\", \"Louisiana\", \"Oklahoma\", \"Texas\"],\n",
      "\"West\"      : [\"Arizona\", \"Colorado\", \"Idaho\", \"Montana\", \"Nevada\", \"New Mexico\", \"Utah\",  \"Wyoming\", \"Alaska\", \"California\", \"Hawaii\", \"Oregon\", \"Washington\"]\n",
      "}"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat ../data/us_states_regions.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'West', u'Northeast', u'Midwest', u'South']\n",
      "[[u'Arizona', u'Colorado', u'Idaho', u'Montana', u'Nevada', u'New Mexico', u'Utah', u'Wyoming', u'Alaska', u'California', u'Hawaii', u'Oregon', u'Washington'], [u'Connecticut', u'Maine', u'Massachusetts', u'New Hampshire', u'Rhode Island', u'Vermont', u'New Jersey', u'New York', u'Pennsylvania'], [u'Illinois', u'Indiana', u'Michigan', u'Ohio', u'Wisconsin', u'Iowa', u'Kansas', u'Minnesota', u'Missouri', u'Nebraska', u'North Dakota', u'South Dakota'], [u'Delaware', u'Florida', u'Georgia', u'Maryland', u'North Carolina', u'South Carolina', u'Virginia', u'District of Columbia', u'West Virginia', u'Alabama', u'Kentucky', u'Mississippi', u'Tennessee', u'Arkansas', u'Louisiana', u'Oklahoma', u'Texas']]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('../data/us_states_regions.json','r') as f:\n",
    "    regions = json.load(f)\n",
    "\n",
    "print regions.keys()\n",
    "print regions.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Idaho</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Montana</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      State Region\n",
       "0   Arizona   West\n",
       "1  Colorado   West\n",
       "2     Idaho   West\n",
       "3   Montana   West\n",
       "4    Nevada   West"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_region = []\n",
    "for key in regions:\n",
    "    _dftmp = pd.DataFrame( regions[key], columns=['State']  )\n",
    "    _dftmp['Region'] = key\n",
    "    df_region.append(_dftmp)\n",
    "    \n",
    "df_region = pd.concat(df_region,ignore_index=True)\n",
    "\n",
    "df_region.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a hash-table (source) to map state name to its abbreviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AZ</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CO</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MT</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NV</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  State Region\n",
       "0    AZ   West\n",
       "1    CO   West\n",
       "2    ID   West\n",
       "3    MT   West\n",
       "4    NV   West"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from util import hash_state_to_abbrev\n",
    "hash_state = hash_state_to_abbrev()\n",
    "\n",
    "df_region['State'] = df_region['State'].map(lambda key: hash_state[key])\n",
    "df_region.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Description</th>\n",
       "      <th>Airport</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10135</td>\n",
       "      <td>Allentown/Bethlehem/Easton, PA: Lehigh Valley ...</td>\n",
       "      <td>Lehigh Valley International</td>\n",
       "      <td>Allentown/Bethlehem/Easton</td>\n",
       "      <td>PA</td>\n",
       "      <td>Northeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10136</td>\n",
       "      <td>Abilene, TX: Abilene Regional</td>\n",
       "      <td>Abilene Regional</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>TX</td>\n",
       "      <td>South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10140</td>\n",
       "      <td>Albuquerque, NM: Albuquerque International Sun...</td>\n",
       "      <td>Albuquerque International Sunport</td>\n",
       "      <td>Albuquerque</td>\n",
       "      <td>NM</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10141</td>\n",
       "      <td>Aberdeen, SD: Aberdeen Regional</td>\n",
       "      <td>Aberdeen Regional</td>\n",
       "      <td>Aberdeen</td>\n",
       "      <td>SD</td>\n",
       "      <td>Midwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10146</td>\n",
       "      <td>Albany, GA: Southwest Georgia Regional</td>\n",
       "      <td>Southwest Georgia Regional</td>\n",
       "      <td>Albany</td>\n",
       "      <td>GA</td>\n",
       "      <td>South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10154</td>\n",
       "      <td>Nantucket, MA: Nantucket Memorial</td>\n",
       "      <td>Nantucket Memorial</td>\n",
       "      <td>Nantucket</td>\n",
       "      <td>MA</td>\n",
       "      <td>Northeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10155</td>\n",
       "      <td>Waco, TX: Waco Regional</td>\n",
       "      <td>Waco Regional</td>\n",
       "      <td>Waco</td>\n",
       "      <td>TX</td>\n",
       "      <td>South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10157</td>\n",
       "      <td>Arcata/Eureka, CA: Arcata</td>\n",
       "      <td>Arcata</td>\n",
       "      <td>Arcata/Eureka</td>\n",
       "      <td>CA</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10158</td>\n",
       "      <td>Atlantic City, NJ: Atlantic City International</td>\n",
       "      <td>Atlantic City International</td>\n",
       "      <td>Atlantic City</td>\n",
       "      <td>NJ</td>\n",
       "      <td>Northeast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10165</td>\n",
       "      <td>Adak Island, AK: Adak</td>\n",
       "      <td>Adak</td>\n",
       "      <td>Adak Island</td>\n",
       "      <td>AK</td>\n",
       "      <td>West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Code                                        Description  \\\n",
       "0  10135  Allentown/Bethlehem/Easton, PA: Lehigh Valley ...   \n",
       "1  10136                      Abilene, TX: Abilene Regional   \n",
       "2  10140  Albuquerque, NM: Albuquerque International Sun...   \n",
       "3  10141                    Aberdeen, SD: Aberdeen Regional   \n",
       "4  10146             Albany, GA: Southwest Georgia Regional   \n",
       "5  10154                  Nantucket, MA: Nantucket Memorial   \n",
       "6  10155                            Waco, TX: Waco Regional   \n",
       "7  10157                          Arcata/Eureka, CA: Arcata   \n",
       "8  10158     Atlantic City, NJ: Atlantic City International   \n",
       "9  10165                              Adak Island, AK: Adak   \n",
       "\n",
       "                             Airport                        City State  \\\n",
       "0        Lehigh Valley International  Allentown/Bethlehem/Easton    PA   \n",
       "1                   Abilene Regional                     Abilene    TX   \n",
       "2  Albuquerque International Sunport                 Albuquerque    NM   \n",
       "3                  Aberdeen Regional                    Aberdeen    SD   \n",
       "4         Southwest Georgia Regional                      Albany    GA   \n",
       "5                 Nantucket Memorial                   Nantucket    MA   \n",
       "6                      Waco Regional                        Waco    TX   \n",
       "7                             Arcata               Arcata/Eureka    CA   \n",
       "8        Atlantic City International               Atlantic City    NJ   \n",
       "9                               Adak                 Adak Island    AK   \n",
       "\n",
       "      Region  \n",
       "0  Northeast  \n",
       "1      South  \n",
       "2       West  \n",
       "3    Midwest  \n",
       "4      South  \n",
       "5  Northeast  \n",
       "6      South  \n",
       "7       West  \n",
       "8  Northeast  \n",
       "9       West  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# good, we're now ready to join this \"Region\" information to our lookup table\n",
    "df_lookup = df_lookup.merge(df_region,on='State',how='left')\n",
    "\n",
    "df_lookup.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add airport latitude and longitude information using geocoder\n",
    "\n",
    "Next we'll query the geograhical location of each airport using geocoders from ``geopy`` ([link](https://geopy.readthedocs.io/en/1.10.0/)).\n",
    "\n",
    "This information will be useful especially when creating visualization plots.\n",
    "\n",
    "The cell below is going to take a while, so good time to brew a coffee... (need to add breaks between API requests to avoid getting timed-out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(  0 out of 334) Elapsed time:  0.00 seconds\n",
      "( 20 out of 334) Elapsed time: 209.90 seconds\n",
      "    lookup failed for: Western Neb. Regional/William B. Heilig Field\n",
      "    lookup failed for: Greater Binghamton/Edwin A. Link Field\n",
      "( 40 out of 334) Elapsed time: 418.37 seconds\n",
      "    lookup failed for: Boise Air Terminal\n",
      "    lookup failed for: Brownsville South Padre Island International\n",
      "    lookup failed for: Baltimore/Washington International Thurgood Marshall\n",
      "    lookup failed for: Akron-Canton Regional\n",
      "( 60 out of 334) Elapsed time: 627.09 seconds\n",
      "    lookup failed for: Charleston AFB/International\n",
      "    lookup failed for: Casper/Natrona County International\n",
      "( 80 out of 334) Elapsed time: 835.74 seconds\n",
      "    lookup failed for: Dickinson - Theodore Roosevelt Regional\n",
      "(100 out of 334) Elapsed time: 1044.65 seconds\n",
      "    lookup failed for: Northwest Florida Beaches International\n",
      "    lookup failed for: Erie International/Tom Ridge Field\n",
      "(120 out of 334) Elapsed time: 1253.41 seconds\n",
      "    lookup failed for: Green Bay Austin Straubel International\n",
      "    lookup failed for: Robert Gray AAF\n",
      "(140 out of 334) Elapsed time: 1463.01 seconds\n",
      "    lookup failed for: Huntsville International-Carl T Jones Field\n",
      "(160 out of 334) Elapsed time: 1672.50 seconds\n",
      "    lookup failed for: Falls International Einarson Field\n",
      "    lookup failed for: Jackson Medgar Wiley Evers International\n",
      "(180 out of 334) Elapsed time: 1881.59 seconds\n",
      "    lookup failed for: Lafayette Regional Paul Fournet Field\n",
      "    lookup failed for: Bill and Hillary Clinton Nat Adams Field\n",
      "(200 out of 334) Elapsed time: 2090.00 seconds\n",
      "(220 out of 334) Elapsed time: 2299.20 seconds\n",
      "    lookup failed for: Modesto City-County-Harry Sham Field\n",
      "    lookup failed for: Dane County Regional-Truax Field\n",
      "    lookup failed for: Louis Armstrong New Orleans International\n",
      "(240 out of 334) Elapsed time: 2508.11 seconds\n",
      "    lookup failed for: Newport News/Williamsburg International\n",
      "(260 out of 334) Elapsed time: 2717.16 seconds\n",
      "    lookup failed for: Petersburg James A Johnson\n",
      "    lookup failed for: Theodore Francis Green State\n",
      "    lookup failed for: Roanoke Blacksburg Regional Woodrum Field\n",
      "    lookup failed for: Roswell International Air Center\n",
      "(280 out of 334) Elapsed time: 2925.91 seconds\n",
      "    lookup failed for: San Angelo Regional/Mathis Field\n",
      "    lookup failed for: Santa Maria Public/Capt. G. Allan Hancock Field\n",
      "(300 out of 334) Elapsed time: 3134.65 seconds\n",
      "    lookup failed for: Francisco C. Ada Saipan International\n",
      "    lookup failed for: Sheppard AFB/Wichita Falls Municipal\n",
      "    lookup failed for: Sioux Gateway/Col. Bud Day Field\n",
      "    lookup failed for: Tri-Cities Regional TN/VA\n",
      "(320 out of 334) Elapsed time: 3343.15 seconds\n",
      "    lookup failed for: Joslin Field - Magic Valley Regional\n",
      "    lookup failed for: Texarkana Regional-Webb Field\n",
      "    lookup failed for: Eglin AFB Destin Fort Walton Beach\n",
      "    lookup failed for: Yuma MCAS/Yuma International\n"
     ]
    }
   ],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "geolocator = Nominatim()\n",
    "\n",
    "t = time.time()\n",
    "lat,lon= [],[]\n",
    "n_items = df_lookup.shape[0]\n",
    "for i,airport in enumerate(df_lookup['Airport']):\n",
    "    if i%20==0:\n",
    "         print '({:3} out of {})'.format(i,n_items),print_time(t)\n",
    "\n",
    "    loc = geolocator.geocode(airport)\n",
    "    time.sleep(10) # add break to avoid api service timeouts\n",
    "\n",
    "    if loc is not None:\n",
    "        lat.append(loc[1][0])\n",
    "        lon.append(loc[1][1])        \n",
    "    else:\n",
    "        print '    lookup failed for: ' + airport\n",
    "        lon.append(None)\n",
    "        lat.append(None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- 36 NANs out 334 (10.78%) --\n"
     ]
    }
   ],
   "source": [
    "# add as new columns\n",
    "df_lookup['lat'] = lat\n",
    "df_lookup['lon'] = lon\n",
    "\n",
    "n_nans = df_lookup['lat'].isnull().sum(axis=0)\n",
    "print \"-- {} NANs out {} ({:.2f}%) --\".format(n_nans,n_items,n_nans/float(n_items)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So some lookup failed...but most succeeded\n",
    "\n",
    "- For airports that failed, search using City + State information (lose locality a bit but will suffice for our analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248     lookup failed for: Newport News/Williamsburg, VA\n",
      "249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302     lookup failed for: Saipan, TT\n",
      "303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333\n"
     ]
    }
   ],
   "source": [
    "idx_nan = [] # keep track of the index location that may fail yet again\n",
    "for i in xrange(n_items):\n",
    "    print i,\n",
    "    if lat[i] is not None:\n",
    "        continue\n",
    "    city,state = df_lookup['City'].ix[i], df_lookup['State'].ix[i]\n",
    "    loc = geolocator.geocode(city+', '+state)\n",
    "    time.sleep(10) # add break to avoid api service timeouts\n",
    "\n",
    "    if loc is not None:\n",
    "        lat[i],lon[i] = loc[1]\n",
    "    else:\n",
    "        print '    lookup failed for: {}, {}'.format(city,state)\n",
    "        idx_nan.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- 2 NANs out 334 (0.60%) --\n"
     ]
    }
   ],
   "source": [
    "# update columns\n",
    "df_lookup['lat'] = lat\n",
    "df_lookup['lon'] = lon\n",
    "\n",
    "n_nans = df_lookup['lat'].isnull().sum(axis=0)\n",
    "print \"-- {} NANs out {} ({:.2f}%) --\".format(n_nans,n_items,n_nans/float(n_items)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File df_lookup_tmp2.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-40d858398027>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_lookup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'df_lookup_tmp2.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\takanori\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\takanori\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\takanori\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 645\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\takanori\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    797\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    798\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 799\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    800\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    801\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\takanori\\Anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas\\parser.c:3427)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas\\parser.c:6861)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File df_lookup_tmp2.csv does not exist"
     ]
    }
   ],
   "source": [
    "df_lookup = pd.read_csv('df_lookup_tmp2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So at this point, we have two lookup failures\n",
    "\n",
    "- Although unelegant, I'll just manually query these in the geocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print idx_nan\n",
    "df_lookup[df_lookup['lat'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "lat,lon = geolocator.geocode('Newport News, VA')[1]\n",
    "df_lookup['lat'].ix[248] = lat\n",
    "df_lookup['lon'].ix[248] = lon\n",
    "\n",
    "lat,lon = geolocator.geocode('Saipan')[1]\n",
    "df_lookup['lat'].ix[302] = lat\n",
    "df_lookup['lon'].ix[302] = lon\n",
    "\n",
    "print df_lookup.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add column with both city and state\n",
    "\n",
    "- Since I am not familiar with many names of the airport, I'd rather work with City and State names.\n",
    "\n",
    "- However, there may be multiple airports in the same city (eg, JKF and Laguardia in NYC), so uniqueness of \"City/State\" is not guaranteed.\n",
    "\n",
    "- Here, I'll create yet another (and final) column containing both the City and State information, and modify duplicates as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df_lookup['City_State'] = df_lookup['City'] + ' (' + df_lookup['State'] + ')'\n",
    "\n",
    "df_lookup.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# check duplicates in \"City_State\"\n",
    "dups = df_lookup['City_State'].value_counts()\n",
    "dups = dups[dups != 1]\n",
    "\n",
    "dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# create hash-table for airport name lookup\n",
    "hash_airport = df_lookup.set_index('Code')['Airport'].to_dict()\n",
    "pprint({k: hash_airport[k] for k in hash_airport.keys()[:5]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# check duplicates in \"City_State\"\n",
    "for dup in dups.index:\n",
    "    print dup\n",
    "    for i in df_lookup[df_lookup['City_State'] == dup].Code:\n",
    "        print \"    (Code = {:6}) Airport = {}\".format((df_data['ORIGIN_AIRPORT_ID'] == i).sum(), hash_airport[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, kinda hacky, but will create manual replacement on these duplicates using the \"cleaner\" below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "cleaner = [\n",
    "    ('Houston (TX) [Ell]', 'Ellington'), \n",
    "    ('Houston (TX) [WP.Hobby]', 'William P Hobby'), \n",
    "    ('Houston (TX) [G.Bush]',  'George Bush Intercontinental/Houston'), \n",
    "    ('Chicago (IL) [Midway]',   'Chicago Midway International'),\n",
    "    (\"Chicago (IL) [O'Hare]\",   \"Chicago O'Hare International\"),\n",
    "    ('Washington (DC) [R.Reagan]',   'Ronald Reagan Washington National'),\n",
    "    ('Washington (DC) [W.Dulles]',   'Washington Dulles International'),\n",
    "    (\"New York (NY) [JFK]\",   \"John F. Kennedy International\"),\n",
    "    (\"New York (NY) [Lag]\",   \"LaGuardia\"),\n",
    "]\n",
    "\n",
    "for _replace, _airport in cleaner:\n",
    "    df_lookup.loc[df_lookup['Airport'] == _airport, 'City_State'] = _replace\n",
    "\n",
    "# check duplicates are removed\n",
    "assert np.all(df_lookup['City_State'].value_counts() == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All done. Save dataframe on disk\n",
    "\n",
    "- We have created our *enhanced* lookup table.\n",
    "\n",
    "- Let's save this on disk for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print df_lookup.shape\n",
    "df_lookup.sample(10).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "df_lookup.to_csv('df_lookup.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "toc_position": {
   "height": "615px",
   "left": "0px",
   "right": "1228px",
   "top": "106px",
   "width": "212px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
